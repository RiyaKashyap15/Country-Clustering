{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d58f2a1",
   "metadata": {},
   "source": [
    "# Clustering Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "397edc46",
   "metadata": {},
   "source": [
    "Problem Statement"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71566251",
   "metadata": {},
   "source": [
    "To categorise the countries using socio-economic and health factors that determine the overall development of the country as HELP International have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. So, CEO has to make decision to choose the countries that are in the direst need of aid.\n",
    "\n",
    "Which countries should receive funding and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8554e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import PCA \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# kmeans clustering \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score,silhouette_samples\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c919ff41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\Github\\\\Untitled Folder'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKING THE WORKING DIRECTORY\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbc02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle file as dataframe\n",
    "dataset = pd.read_pickle('country_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bd4565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECKING THE ROWS AND COLUMNS OF THE DATASET\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07b7c5",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Why scale the data in this case?\n",
    "\n",
    "the features have incomparable units (metrics are percentages, dollar values, whole numbers)\n",
    "the range values of the features also vary (one for example is 0 to 200, and another 0 to 100,000)\n",
    "this level of variance can negatively impact the performance of this model,as this model is based on measuring distances, it can do this by giving more weight to some features\n",
    "by scaling we are removing potential bias that the model can have towards features with higher magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns argument ==> we'll use this later to create a new dataframe with the rescaled data \n",
    "columns = dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc0c51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Afghanistan'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 'scaler' is for the rescaling technique, 'fit' function is to find the x_min and the x_max, 'transform' function applies formula to all elements of data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m rescaled_dataset_minmax \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m rescaled_dataset_minmax\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:420\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:457\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    465\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Afghanistan'"
     ]
    }
   ],
   "source": [
    "#Scale the data: MinMaxScaler (normalised)\n",
    "\n",
    "# the scaler to use will be \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 'scaler' is for the rescaling technique, 'fit' function is to find the x_min and the x_max, 'transform' function applies formula to all elements of data\n",
    "rescaled_dataset_minmax = scaler.fit_transform(dataset)\n",
    "rescaled_dataset_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the data: StandardScaler (standardised)\n",
    "# in standardisation, all features will be transformed to have the properties of standard normal distribution with mean=0 and standard deviation=1\n",
    "# the scaler to use will be \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 'scaler' is for the rescaling technique, 'fit' function is to find the x_min and the x_max, 'transform' function applies formula to all elements of data\n",
    "rescaled_dataset_standard = scaler.fit_transform(dataset)\n",
    "rescaled_dataset_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ba6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled dataframes\n",
    "# minmax\n",
    "# we need to create a new dataframe with the column lables and the rescaled values \n",
    "df_minmax = pd.DataFrame(data= rescaled_dataset_minmax , columns = columns )\n",
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df74337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation\n",
    "# we need to create a new dataframe with the column lables and the rescaled values \n",
    "df_standard = pd.DataFrame(data= rescaled_dataset_standard , columns = columns)\n",
    "df_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a38b7e",
   "metadata": {},
   "source": [
    "## PCA: Principal Component Analysis\n",
    "PCA with data scaled with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0325da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform\n",
    "pca = PCA()\n",
    "pca.fit(df_standard)\n",
    "pca_data_standard = pca.transform(df_standard)\n",
    "\n",
    "# percentage variation \n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals =1)\n",
    "labels = ['PC' + str(x) for x in range (1, len(per_var)+1)]\n",
    "\n",
    "# plot the percentage of explained variance by principal component\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# plot pca\n",
    "pca_df_standard = pd.DataFrame(pca_data_standard, columns = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f8a1b",
   "metadata": {},
   "source": [
    "## PCA with data scaled with MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform\n",
    "pca = PCA()\n",
    "pca.fit(df_minmax)\n",
    "pca_data_minmax = pca.transform(df_minmax)\n",
    "\n",
    "# percentage variation \n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals =1)\n",
    "labels = ['PC' + str(x) for x in range (1, len(per_var)+1)]\n",
    "\n",
    "# plot the percentage of explained variance by principal component\n",
    "plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "# plot pca\n",
    "pca_df_minmax = pd.DataFrame(pca_data_minmax, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8694f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with PC1, PC2, P3, PC4\n",
    "data2 = pca_df_standard.drop(['PC5','PC6','PC7','PC8','PC9'], axis = 1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a77c57",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "\n",
    "After doing PCA with both standardised and normalised versions of the original dataset, we can see that there are 4 principal components can explain about 90% of the distribution of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe19bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea8145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: K-Means Clustering\n",
    "\n",
    "km = KMeans (\n",
    "    n_clusters = 3, # number of clusters/centroids to create\n",
    "    init = 'random', # ‘random’: choose n_clusters observations (rows) at random from data for the initial centroids\n",
    "    n_init = 10, # this is the default value. This is the number of times the k-means algorithm will be run with different centroid seeds\n",
    "    max_iter = 300, # this is the default value. This is the maximum number of iterations of the k-means algorithm for a single run.\n",
    "    tol = 1e-4, # this is the default value. This is the relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.\n",
    "    random_state = 0 # this is the default value. Determines random number generation for centroid initialization. Use an int to make the randomness deterministic.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29722c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
